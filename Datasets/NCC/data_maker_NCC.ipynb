{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff69aa7",
   "metadata": {},
   "source": [
    "- This file is used for making train data (1) and test data (5) from all sensor\n",
    "- The training data will consist of an accumulation of 70% from each sensor\n",
    "- The test data will consist of 30% from each sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d49201dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:22:29.560626Z",
     "iopub.status.busy": "2026-01-16T11:22:29.560626Z",
     "iopub.status.idle": "2026-01-16T11:22:30.356247Z",
     "shell.execute_reply": "2026-01-16T11:22:30.356247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting combined data processing and encoding script [NCC v15 - Standalone, Target Dtypes]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "print(\"Starting combined data processing and encoding script [NCC v15 - Standalone, Target Dtypes]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b94b64f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:22:30.356247Z",
     "iopub.status.busy": "2026-01-16T11:22:30.356247Z",
     "iopub.status.idle": "2026-01-16T11:22:30.360261Z",
     "shell.execute_reply": "2026-01-16T11:22:30.360261Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = 'final_dataset'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "# Definition of the dataset file to be loaded\n",
    "dataset_files = {\n",
    "    '1': 'scenario_dataset_1/dataset_result.binetflow',\n",
    "    '2': 'scenario_dataset_2/dataset_result.binetflow',\n",
    "    '5': 'scenario_dataset_5/dataset_result.binetflow',\n",
    "    '9': 'scenario_dataset_9/dataset_result.binetflow',\n",
    "    '13': 'scenario_dataset_13/dataset_result.binetflow'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40393057",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:22:30.360261Z",
     "iopub.status.busy": "2026-01-16T11:22:30.360261Z",
     "iopub.status.idle": "2026-01-16T11:22:30.405093Z",
     "shell.execute_reply": "2026-01-16T11:22:30.405093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all raw NCC dataframes for encoder fitting...\n",
      "Successfully loaded: scenario_dataset_1/dataset_result.binetflow (key: 1, shape: (10, 15))\n",
      "Successfully loaded: scenario_dataset_2/dataset_result.binetflow (key: 2, shape: (10, 15))\n",
      "Successfully loaded: scenario_dataset_5/dataset_result.binetflow (key: 5, shape: (10, 15))\n",
      "Successfully loaded: scenario_dataset_9/dataset_result.binetflow (key: 9, shape: (10, 15))\n",
      "Successfully loaded: scenario_dataset_13/dataset_result.binetflow (key: 13, shape: (10, 15))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list = []\n",
    "loaded_keys = []\n",
    "all_original_data = []\n",
    "\n",
    "print(\"Loading all raw NCC dataframes for encoder fitting...\")\n",
    "for key, path in dataset_files.items():\n",
    "    try:\n",
    "        # Specify specific dtypes for problematic columns\n",
    "        dtype_spec = {'sTos': str, 'dTos': str}\n",
    "        df = pd.read_csv(path, dtype=dtype_spec)\n",
    "        \n",
    "        # Special handling for sTos and dTos after being loaded as strings\n",
    "        if 'dTos' in df.columns:\n",
    "            df['dTos'] = pd.to_numeric(df['dTos'], errors='coerce').fillna(0)\n",
    "        if 'sTos' in df.columns:\n",
    "            df['sTos'] = pd.to_numeric(df['sTos'], errors='coerce').fillna(0)\n",
    "            \n",
    "        df_list.append(df)\n",
    "        all_original_data.append(df.copy()) # Create a copy for encoder fitting\n",
    "        loaded_keys.append(key)\n",
    "        print(f\"Successfully loaded: {path} (key: {key}, shape: {df.shape})\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Dataset file not found at {path}. Skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load {path}. Error: {e}. Skipping.\")\n",
    "\n",
    "if not df_list:\n",
    "    print(\"Error: No .binetflow datasets could be loaded. Stopping script.\")\n",
    "    # In a notebook, you may want to raise an error instead of using exit()\n",
    "    # raise IOError(\"No .binetflow datasets could be loaded.\")\n",
    "\n",
    "del dtype_spec\n",
    "del dataset_files\n",
    "del path\n",
    "del key\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948c3ec4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:22:30.405093Z",
     "iopub.status.busy": "2026-01-16T11:22:30.405093Z",
     "iopub.status.idle": "2026-01-16T11:22:30.499043Z",
     "shell.execute_reply": "2026-01-16T11:22:30.499043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LabelEncoders on combined raw NCC data...\n",
      "Fitting LabelEncoder for: SrcAddr\n",
      "  No NaN found, using fallback ID: 0\n",
      "Fitting LabelEncoder for: Sport\n",
      "  No NaN found, using fallback ID: 0\n",
      "Fitting LabelEncoder for: Dir\n",
      "  No NaN found, using fallback ID: 0\n",
      "Fitting LabelEncoder for: DstAddr\n",
      "  No NaN found, using fallback ID: 0\n",
      "Fitting LabelEncoder for: Dport\n",
      "  No NaN found, using fallback ID: 0\n",
      "Fitting LabelEncoder for: State\n",
      "  No NaN found, using fallback ID: 0\n",
      "Encoders are ready.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Fitting LabelEncoders on combined raw NCC data...\")\n",
    "combined_fit_df = pd.concat(all_original_data, ignore_index=True)\n",
    "del all_original_data # Delete raw merged copies as soon as possible\n",
    "gc.collect()\n",
    "\n",
    "categorical_cols = ['SrcAddr', 'Sport', 'Dir', 'DstAddr', 'Dport', 'State']\n",
    "encoders = {}\n",
    "imputation_values = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in combined_fit_df.columns:\n",
    "        print(f\"Fitting LabelEncoder for: {col}\")\n",
    "        # Fill NaN with the string 'nan' so it can be encodeds\n",
    "        combined_fit_df[col] = combined_fit_df[col].fillna('nan').astype(str)\n",
    "        unique_values = combined_fit_df[col].unique()\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        le.fit(unique_values)\n",
    "        encoders[col] = le\n",
    "        \n",
    "        # Save the imputation value for 'nan'\n",
    "        if 'nan' in le.classes_:\n",
    "            imputation_values[col] = int(le.transform(['nan'])[0])\n",
    "            print(f\"  Found NaN, its encoded ID is: {imputation_values[col]}\")\n",
    "        else:\n",
    "            # Fallback if 'nan' is not found (rarely occurs)\n",
    "            imputation_values[col] = 0 \n",
    "            print(f\"  No NaN found, using fallback ID: {imputation_values[col]}\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found in combined data. Skipping encoding for it.\")\n",
    "\n",
    "print(\"Encoders are ready.\")\n",
    "del combined_fit_df\n",
    "gc.collect()\n",
    "\n",
    "del unique_values\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab4fdc",
   "metadata": {},
   "source": [
    "To make the data simpler, (following my paper) this code will remove the unnecessary feature such as:\n",
    "- dTos\n",
    "- sTos\n",
    "- ActivityLabel (only in NCC-2)\n",
    "- BotnetName (only in NCC-2)\n",
    "- SensorId (only in NCC-2)\n",
    "- StartTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23a85efd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:22:30.499043Z",
     "iopub.status.busy": "2026-01-16T11:22:30.499043Z",
     "iopub.status.idle": "2026-01-16T11:22:30.536715Z",
     "shell.execute_reply": "2026-01-16T11:22:30.536715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied label simplification.\n",
      "Dropped unnecessary columns (if they existed).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Define the label categorization function\n",
    "def categorize_label(label):\n",
    "    label_str = str(label).lower()\n",
    "    if 'botnet' in label_str:\n",
    "        if 'spam' in label_str: return 'botnet_spam'\n",
    "        else: return 'botnet'\n",
    "    elif 'background' in label_str or 'normal' in label_str: return 'normal'\n",
    "    else: return 'normal'\n",
    "\n",
    "# 2. Apply the label categorization\n",
    "for i in range(len(df_list)):\n",
    "    df_list[i]['Label'] = df_list[i]['Label'].apply(categorize_label)\n",
    "print(\"Applied label simplification.\")\n",
    "\n",
    "# 3. Drop unnecessary columns\\\n",
    "columns_to_drop = ['dTos', 'sTos', 'ActivityLabel', 'BotnetName', 'SensorId', 'StartTime']\n",
    "for i in range(len(df_list)):\n",
    "    cols_to_drop_existing = [col for col in columns_to_drop if col in df_list[i].columns]\n",
    "    if cols_to_drop_existing:\n",
    "        df_list[i] = df_list[i].drop(columns=cols_to_drop_existing, errors='ignore')\n",
    "print(f\"Dropped unnecessary columns (if they existed).\")\n",
    "\n",
    "del columns_to_drop\n",
    "del cols_to_drop_existing\n",
    "del col, i\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c9d1f57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:22:30.538226Z",
     "iopub.status.busy": "2026-01-16T11:22:30.538226Z",
     "iopub.status.idle": "2026-01-16T11:22:30.638399Z",
     "shell.execute_reply": "2026-01-16T11:22:30.638399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed stratified train-test split."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_train, normal_test = [], []\n",
    "botnet_train, botnet_test = [], []\n",
    "botnet_spam_train, botnet_spam_test = [], []\n",
    "normal_df, botnet_df, botnet_spam_df = [], [], []\n",
    "\n",
    "# Separate by label\n",
    "for df in df_list:\n",
    "    normal_df.append(df[df['Label'] == 'normal'])\n",
    "    botnet_df.append(df[df['Label'] == 'botnet'])\n",
    "    botnet_spam_df.append(df[df['Label'] == 'botnet_spam'])\n",
    "del df_list\n",
    "gc.collect()\n",
    "\n",
    "# Perform split for each file and each label\n",
    "for i in range(len(loaded_keys)):\n",
    "    if i < len(normal_df) and not normal_df[i].empty:\n",
    "        tr, te = train_test_split(normal_df[i], test_size=0.3, random_state=42)\n",
    "        normal_train.append(tr); normal_test.append(te)\n",
    "    else:\n",
    "        normal_train.append(pd.DataFrame()); normal_test.append(pd.DataFrame())\n",
    "        \n",
    "    if i < len(botnet_df) and not botnet_df[i].empty:\n",
    "        tr, te = train_test_split(botnet_df[i], test_size=0.3, random_state=42)\n",
    "        botnet_train.append(tr); botnet_test.append(te)\n",
    "    else:\n",
    "        botnet_train.append(pd.DataFrame()); botnet_test.append(pd.DataFrame())\n",
    "\n",
    "    if i < len(botnet_spam_df) and not botnet_spam_df[i].empty:\n",
    "        tr, te = train_test_split(botnet_spam_df[i], test_size=0.3, random_state=42)\n",
    "        botnet_spam_train.append(tr); botnet_spam_test.append(te)\n",
    "    else:\n",
    "        botnet_spam_train.append(pd.DataFrame()); botnet_spam_test.append(pd.DataFrame())\n",
    "\n",
    "print(\"Performed stratified train-test split.\")\n",
    "del normal_df, botnet_df, botnet_spam_df\n",
    "gc.collect()\n",
    "\n",
    "del tr, te, i\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c89adc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:22:30.638399Z",
     "iopub.status.busy": "2026-01-16T11:22:30.638399Z",
     "iopub.status.idle": "2026-01-16T11:22:30.703071Z",
     "shell.execute_reply": "2026-01-16T11:22:30.703071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined training data (shape: (30, 12))\n"
     ]
    }
   ],
   "source": [
    "temp_train_df = []\n",
    "num_files = len(loaded_keys)\n",
    "\n",
    "# Merge back by file\n",
    "for i in range(num_files):\n",
    "    file_dfs = []\n",
    "    if i < len(normal_train) and not normal_train[i].empty: file_dfs.append(normal_train[i])\n",
    "    if i < len(botnet_train) and not botnet_train[i].empty: file_dfs.append(botnet_train[i])\n",
    "    if i < len(botnet_spam_train) and not botnet_spam_train[i].empty: file_dfs.append(botnet_spam_train[i])\n",
    "    \n",
    "    if file_dfs:\n",
    "        temp_train_df.append(pd.concat(file_dfs, ignore_index=True))\n",
    "        \n",
    "del normal_train, botnet_train, botnet_spam_train\n",
    "gc.collect()\n",
    "\n",
    "if not temp_train_df:\n",
    "     print(\"Error: No training data was generated.\")\n",
    "     # raise Exception(\"No training data generated\")\n",
    "else:\n",
    "    train_df = pd.concat(temp_train_df, ignore_index=True)\n",
    "    del temp_train_df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Shuffle the training data\n",
    "    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Combined training data (shape: {train_df.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14687d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:22:30.703071Z",
     "iopub.status.busy": "2026-01-16T11:22:30.703071Z",
     "iopub.status.idle": "2026-01-16T11:22:30.771372Z",
     "shell.execute_reply": "2026-01-16T11:22:30.771372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined test data (number of test sets: 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dfs = {}\n",
    "for i in range(num_files):\n",
    "    file_dfs = []\n",
    "    if i < len(normal_test) and not normal_test[i].empty: file_dfs.append(normal_test[i])\n",
    "    if i < len(botnet_test) and not botnet_test[i].empty: file_dfs.append(botnet_test[i])\n",
    "    if i < len(botnet_spam_test) and not botnet_spam_test[i].empty: file_dfs.append(botnet_spam_test[i])\n",
    "    \n",
    "    if file_dfs:\n",
    "        df = pd.concat(file_dfs, ignore_index=True)\n",
    "        # Shuffle each test data set\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        test_dfs[loaded_keys[i]] = df\n",
    "        \n",
    "del normal_test, botnet_test, botnet_spam_test\n",
    "gc.collect()\n",
    "print(f\"Combined test data (number of test sets: {len(test_dfs)})\")\n",
    "\n",
    "del i\n",
    "del file_dfs\n",
    "del df\n",
    "del num_files\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "453536be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:22:30.773235Z",
     "iopub.status.busy": "2026-01-16T11:22:30.773235Z",
     "iopub.status.idle": "2026-01-16T11:22:31.112137Z",
     "shell.execute_reply": "2026-01-16T11:22:31.112137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying transformations (Proto dummies, fitted Encoders) to all datasets...\n",
      "Processing DataFrame partition (Initial shape: (30, 12))\n",
      "  Shape after get_dummies: (30, 13)\n",
      "  Applying target data types...\n",
      "  Shape after finalizing columns and types: (30, 28)\n",
      "Processing DataFrame partition (Initial shape: (4, 12))\n",
      "  Shape after get_dummies: (4, 13)\n",
      "  Applying target data types...\n",
      "  Shape after finalizing columns and types: (4, 28)\n",
      "Processing DataFrame partition (Initial shape: (4, 12))\n",
      "  Shape after get_dummies: (4, 13)\n",
      "  Applying target data types...\n",
      "  Shape after finalizing columns and types: (4, 28)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame partition (Initial shape: (4, 12))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape after get_dummies: (4, 13)\n",
      "  Applying target data types...\n",
      "  Shape after finalizing columns and types: (4, 28)\n",
      "Processing DataFrame partition (Initial shape: (4, 12))\n",
      "  Shape after get_dummies: (4, 13)\n",
      "  Applying target data types...\n",
      "  Shape after finalizing columns and types: (4, 28)\n",
      "Processing DataFrame partition (Initial shape: (4, 12))\n",
      "  Shape after get_dummies: (4, 13)\n",
      "  Applying target data types...\n",
      "  Shape after finalizing columns and types: (4, 28)\n",
      "Transformations complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dfs_to_transform = [train_df] + list(test_dfs.values())\n",
    "del train_df # Remove old references, we will process from the list\n",
    "gc.collect()\n",
    "\n",
    "final_dfs_list = []\n",
    "\n",
    "# --- Define the target column list ---\n",
    "protocol_cols_from_target = [\n",
    "    'arp', 'esp', 'icmp', 'igmp', 'ipv6', 'ipv6-icmp', 'ipx/spx', 'llc',\n",
    "    'pim', 'rarp', 'rtcp', 'rtp', 'tcp', 'udp', 'udt', 'unas', 'ipnip'\n",
    "]\n",
    "core_cols = ['Dur', 'SrcAddr', 'Sport', 'Dir', 'DstAddr', 'Dport', 'State',\n",
    "             'TotPkts', 'TotBytes', 'SrcBytes', 'Label']\n",
    "final_column_order = core_cols + protocol_cols_from_target\n",
    "# ------------------------------------\n",
    "\n",
    "print(\"Applying transformations (Proto dummies, fitted Encoders) to all datasets...\")\n",
    "for df_orig in all_dfs_to_transform:\n",
    "    df = df_orig.copy()\n",
    "    print(f\"Processing DataFrame partition (Initial shape: {df.shape})\")\n",
    "\n",
    "    # 1. One-hot encoding 'Proto'\n",
    "    if 'Proto' in df.columns:\n",
    "        df = pd.get_dummies(df, columns=['Proto'], prefix='', prefix_sep='')\n",
    "        print(f\"  Shape after get_dummies: {df.shape}\")\n",
    "\n",
    "    # 2. Apply the pre-fitted LabelEncoder\n",
    "    for col in categorical_cols:\n",
    "         if col in encoders:\n",
    "            df[col] = df[col].fillna('nan').astype(str)\n",
    "            # Separate known and unknown values\n",
    "            known_mask = df[col].isin(encoders[col].classes_)\n",
    "            known_indices = df.index[known_mask]\n",
    "            unknown_indices = df.index[~known_mask]\n",
    "            \n",
    "            # Transform known values\n",
    "            if not known_indices.empty:\n",
    "                df.loc[known_indices, col] = encoders[col].transform(df.loc[known_indices, col])\n",
    "            \n",
    "            # Fill unknown values with imputed value 'nan'\n",
    "            unknown_fill_value = imputation_values.get(col, 0)\n",
    "            if not unknown_indices.empty:\n",
    "                df.loc[unknown_indices, col] = unknown_fill_value\n",
    "         elif col in df.columns: # If the column exists but there is no encoder (this should not happen)\n",
    "            df[col] = imputation_values.get(col, 0)\n",
    "\n",
    "    # 3. Ensure all columns (including protocol) are present\n",
    "    for col in final_column_order:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0 # Add missing protocol/other columns as 0\n",
    "\n",
    "    # 4. Set column order and select only the necessary ones\n",
    "    df_final = df[final_column_order].copy()\n",
    "\n",
    "    # 5. Apply the target data type\n",
    "    print(f\"  Applying target data types...\")\n",
    "    for col in df_final.columns:\n",
    "        if col == 'Dur':\n",
    "            df_final[col] = pd.to_numeric(df_final[col], errors='coerce').fillna(0).astype(np.float64)\n",
    "        elif col == 'Label':\n",
    "            df_final[col] = df_final[col].astype(object) # Label remains as string\n",
    "        else:\n",
    "            # All other feature columns (including ID encode) should be int64\n",
    "            df_final[col] = pd.to_numeric(df_final[col], errors='coerce').fillna(0).astype(np.int64)\n",
    "\n",
    "    print(f\"  Shape after finalizing columns and types: {df_final.shape}\")\n",
    "    final_dfs_list.append(df_final)\n",
    "    \n",
    "    del df, df_orig, df_final\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Transformations complete.\")\n",
    "\n",
    "del all_dfs_to_transform\n",
    "del col, categorical_cols, known_mask, known_indices, unknown_indices, unknown_fill_value, imputation_values\n",
    "del protocol_cols_from_target, core_cols, final_column_order\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6f3935d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:22:31.112137Z",
     "iopub.status.busy": "2026-01-16T11:22:31.112137Z",
     "iopub.status.idle": "2026-01-16T11:22:31.122594Z",
     "shell.execute_reply": "2026-01-16T11:22:31.122594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ENCODED training data to final_dataset\\train.csv (shape: (30, 28))\n",
      "Saved ENCODED test data to final_dataset\\test_1.csv (shape: (4, 28))\n",
      "Saved ENCODED test data to final_dataset\\test_2.csv (shape: (4, 28))\n",
      "Saved ENCODED test data to final_dataset\\test_5.csv (shape: (4, 28))\n",
      "Saved ENCODED test data to final_dataset\\test_9.csv (shape: (4, 28))\n",
      "Saved ENCODED test data to final_dataset\\test_13.csv (shape: (4, 28))\n",
      "\n",
      "--- All-in-one processing and encoding complete for NCC! ---\n",
      "Final files are in the 'final_dataset' directory.\n"
     ]
    }
   ],
   "source": [
    "# Separate the training and test data from the list again\n",
    "final_train_df = final_dfs_list[0]\n",
    "final_test_dfs = {key: df for key, df in zip(test_dfs.keys(), final_dfs_list[1:])}\n",
    "\n",
    "# Save the training data\n",
    "train_output_file = os.path.join(output_dir, 'train.csv')\n",
    "final_train_df.to_csv(train_output_file, index=False)\n",
    "print(f\"Saved ENCODED training data to {train_output_file} (shape: {final_train_df.shape})\")\n",
    "\n",
    "# Save all test data\n",
    "for key, df in final_test_dfs.items():\n",
    "    test_output_file = os.path.join(output_dir, f\"test_{key}.csv\")\n",
    "    df.to_csv(test_output_file, index=False)\n",
    "    print(f\"Saved ENCODED test data to {test_output_file} (shape: {df.shape})\")\n",
    "\n",
    "print(\"\\n--- All-in-one processing and encoding complete for NCC! ---\")\n",
    "print(f\"Final files are in the '{output_dir}' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
