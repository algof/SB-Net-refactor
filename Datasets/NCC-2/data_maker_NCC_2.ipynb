{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "476dd79d",
   "metadata": {},
   "source": [
    "- This file is used for making train data (1) and test data (3) from all sensor\n",
    "- The training data will consist of an accumulation of 70% from each sensor\n",
    "- The test data will consist of 30% from each sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed1487d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing and encoding script...\n",
      "Created output directory: final_dataset\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "print(\"Starting data processing and encoding script...\")\n",
    "\n",
    "# Set up the output directory\n",
    "output_dir = 'final_dataset'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "# Define dataset file locations\n",
    "dataset_files = {\n",
    "    '1': 'sensor1/sensor1.binetflow',\n",
    "    '2': 'sensor2/sensor2.binetflow',\n",
    "    '3': 'sensor3/sensor3.binetflow',\n",
    "}\n",
    "\n",
    "# Initialize lists to store dataframes\n",
    "df_list = []\n",
    "loaded_keys = []\n",
    "all_original_data = [] # For fitting the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4b17d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all raw sensor data for encoder fitting...\n",
      "Successfully loaded: sensor1/sensor1.binetflow (key: 1, shape: (4895158, 18))\n",
      "Successfully loaded: sensor2/sensor2.binetflow (key: 2, shape: (5998133, 18))\n",
      "Successfully loaded: sensor3/sensor3.binetflow (key: 3, shape: (3885792, 18))\n",
      "Total 3 datasets successfully loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading all raw sensor data for encoder fitting...\")\n",
    "for key, path in dataset_files.items():\n",
    "    try:\n",
    "        # Dtype specification for problematic columns\n",
    "        dtype_spec = {'sTos': str, 'dTos': str}\n",
    "        df = pd.read_csv(path, dtype=dtype_spec)\n",
    "        \n",
    "        # Manual handling for sTos and dTos columns\n",
    "        if 'dTos' in df.columns:\n",
    "            df['dTos'] = pd.to_numeric(df['dTos'], errors='coerce').fillna(0)\n",
    "        if 'sTos' in df.columns:\n",
    "            df['sTos'] = pd.to_numeric(df['sTos'], errors='coerce').fillna(0)\n",
    "            \n",
    "        df_list.append(df)\n",
    "        all_original_data.append(df.copy()) # Save a raw copy for fitting\n",
    "        loaded_keys.append(key)\n",
    "        print(f\"Successfully loaded: {path} (key: {key}, shape: {df.shape})\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Dataset file not found at {path}. Skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load {path}. Error: {e}. Skipping.\")\n",
    "\n",
    "# Stop the script if no data was successfully loaded\n",
    "if not df_list:\n",
    "    print(\"Error: No .binetflow datasets could be loaded. Stopping script.\")\n",
    "    # In a notebook, you might want to raise an Error instead of exit()\n",
    "    # raise Exception(\"No data could be loaded\")\n",
    "else:\n",
    "    print(f\"Total {len(df_list)} datasets successfully loaded.\")\n",
    "    \n",
    "del dtype_spec\n",
    "del dataset_files\n",
    "del path\n",
    "del key\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f888ace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LabelEncoders on combined raw sensor data...\n",
      "Fitting LabelEncoder for: SrcAddr\n",
      "  No NaN found, using fallback ID: 0\n",
      "Fitting LabelEncoder for: Sport\n",
      "  Found NaN, its encoded ID is: 74919\n",
      "Fitting LabelEncoder for: Dir\n",
      "  No NaN found, using fallback ID: 0\n",
      "Fitting LabelEncoder for: DstAddr\n",
      "  No NaN found, using fallback ID: 0\n",
      "Fitting LabelEncoder for: Dport\n",
      "  Found NaN, its encoded ID is: 92637\n",
      "Fitting LabelEncoder for: State\n",
      "  Found NaN, its encoded ID is: 405\n",
      "All encoders are ready.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Fitting LabelEncoders on combined raw sensor data...\")\n",
    "combined_fit_df = pd.concat(all_original_data, ignore_index=True)\n",
    "\n",
    "# Delete the raw data list to save memory\n",
    "del all_original_data\n",
    "gc.collect()\n",
    "\n",
    "categorical_cols = ['SrcAddr', 'Sport', 'Dir', 'DstAddr', 'Dport', 'State']\n",
    "encoders = {}\n",
    "imputation_values = {} # To store the encoded value of 'nan'\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in combined_fit_df.columns:\n",
    "        print(f\"Fitting LabelEncoder for: {col}\")\n",
    "        # Ensure 'nan' is recognized as a string\n",
    "        combined_fit_df[col] = combined_fit_df[col].fillna('nan').astype(str)\n",
    "        unique_values = combined_fit_df[col].unique()\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        le.fit(unique_values)\n",
    "        encoders[col] = le\n",
    "        \n",
    "        # Save the encoding value for 'nan'\n",
    "        if 'nan' in le.classes_:\n",
    "            imputation_values[col] = int(le.transform(['nan'])[0])\n",
    "            print(f\"  Found NaN, its encoded ID is: {imputation_values[col]}\")\n",
    "        else:\n",
    "            # If 'nan' was not present during fitting, use 0 as a fallback\n",
    "            imputation_values[col] = 0 \n",
    "            print(f\"  No NaN found, using fallback ID: {imputation_values[col]}\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found. Skipping encoding for it.\")\n",
    "\n",
    "print(\"All encoders are ready.\")\n",
    "\n",
    "# Delete the large combined dataframe\n",
    "del combined_fit_df\n",
    "gc.collect()\n",
    "\n",
    "del unique_values\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d2d45b",
   "metadata": {},
   "source": [
    "To make the data simpler, (following my paper) this code will remove the unnecessary feature such as:\n",
    "- dTos\n",
    "- sTos\n",
    "- ActivityLabel (only in NCC-2)\n",
    "- BotnetName (only in NCC-2)\n",
    "- SensorId (only in NCC-2)\n",
    "- StartTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "451867b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label simplification complete.\n",
      "Unnecessary columns have been dropped.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Define the label categorization function\n",
    "def categorize_label(label):\n",
    "    label_str = str(label).lower()\n",
    "    if 'botnet' in label_str:\n",
    "        if 'spam' in label_str: return 'botnet_spam'\n",
    "        else: return 'botnet'\n",
    "    elif 'background' in label_str or 'normal' in label_str: return 'normal'\n",
    "    else: return 'normal' # Assume other labels as normal\n",
    "\n",
    "# 2. Apply the function to all dataframes in df_list\n",
    "for i in range(len(df_list)):\n",
    "    df_list[i]['Label'] = df_list[i]['Label'].apply(categorize_label)\n",
    "print(\"Label simplification complete.\")\n",
    "\n",
    "# 3. Drop unnecessary columns\n",
    "columns_to_drop = ['dTos', 'sTos', 'ActivityLabel', 'BotnetName', 'SensorId', 'StartTime']\n",
    "for i in range(len(df_list)):\n",
    "    cols_to_drop_existing = [col for col in columns_to_drop if col in df_list[i].columns]\n",
    "    if cols_to_drop_existing:\n",
    "        df_list[i] = df_list[i].drop(columns=cols_to_drop_existing, errors='ignore')\n",
    "print(f\"Unnecessary columns have been dropped.\")\n",
    "\n",
    "del columns_to_drop\n",
    "del cols_to_drop_existing\n",
    "del col, i\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ce5b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-file stratified train-test split process complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize lists to hold the split results\n",
    "normal_train, normal_test = [], []\n",
    "botnet_train, botnet_test = [], []\n",
    "botnet_spam_train, botnet_spam_test = [], []\n",
    "normal_df, botnet_df, botnet_spam_df = [], [], []\n",
    "\n",
    "# 1. Separate each dataframe by label\n",
    "for df in df_list:\n",
    "    normal_df.append(df[df['Label'] == 'normal'])\n",
    "    botnet_df.append(df[df['Label'] == 'botnet'])\n",
    "    botnet_spam_df.append(df[df['Label'] == 'botnet_spam'])\n",
    "    \n",
    "del df_list # df_list is no longer needed\n",
    "gc.collect()\n",
    "\n",
    "# 2. Perform train_test_split on each category from each file\n",
    "for i in range(len(loaded_keys)):\n",
    "    # Split Normal\n",
    "    if i < len(normal_df) and not normal_df[i].empty:\n",
    "        tr, te = train_test_split(normal_df[i], test_size=0.3, random_state=42)\n",
    "        normal_train.append(tr); normal_test.append(te)\n",
    "    else:\n",
    "        normal_train.append(pd.DataFrame()); normal_test.append(pd.DataFrame())\n",
    "        \n",
    "    # Split Botnet\n",
    "    if i < len(botnet_df) and not botnet_df[i].empty:\n",
    "        tr, te = train_test_split(botnet_df[i], test_size=0.3, random_state=42)\n",
    "        botnet_train.append(tr); botnet_test.append(te)\n",
    "    else:\n",
    "        botnet_train.append(pd.DataFrame()); botnet_test.append(pd.DataFrame())\n",
    "\n",
    "    # Split Botnet_Spam\n",
    "    if i < len(botnet_spam_df) and not botnet_spam_df[i].empty:\n",
    "        tr, te = train_test_split(botnet_spam_df[i], test_size=0.3, random_state=42)\n",
    "        botnet_spam_train.append(tr); botnet_spam_test.append(te)\n",
    "    else:\n",
    "        botnet_spam_train.append(pd.DataFrame()); botnet_spam_test.append(pd.DataFrame())\n",
    "\n",
    "print(\"Per-file stratified train-test split process complete.\")\n",
    "\n",
    "# Delete intermediate dataframes\n",
    "del normal_df, botnet_df, botnet_spam_df\n",
    "gc.collect()\n",
    "\n",
    "del tr, te, i\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "755d5b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined training data created (shape: (10345357, 12))\n"
     ]
    }
   ],
   "source": [
    "temp_train_df = []\n",
    "num_files = len(loaded_keys)\n",
    "\n",
    "# Combine all train categories per file\n",
    "for i in range(num_files):\n",
    "    file_dfs = []\n",
    "    if i < len(normal_train) and not normal_train[i].empty: file_dfs.append(normal_train[i])\n",
    "    if i < len(botnet_train) and not botnet_train[i].empty: file_dfs.append(botnet_train[i])\n",
    "    if i < len(botnet_spam_train) and not botnet_spam_train[i].empty: file_dfs.append(botnet_spam_train[i])\n",
    "    \n",
    "    if file_dfs:\n",
    "        temp_train_df.append(pd.concat(file_dfs, ignore_index=True))\n",
    "\n",
    "del normal_train, botnet_train, botnet_spam_train\n",
    "gc.collect()\n",
    "\n",
    "if not temp_train_df:\n",
    "    print(\"Error: No training data was generated.\")\n",
    "    # raise Exception(\"No training data\")\n",
    "else:\n",
    "    # Combine all training data from all files into one\n",
    "    train_df = pd.concat(temp_train_df, ignore_index=True)\n",
    "    del temp_train_df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Shuffle the training data\n",
    "    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Combined training data created (shape: {train_df.shape})\")\n",
    "    # You can add train_df.head() here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e17841e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined test data created (number of test sets: 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dfs = {} # Dictionary to store test sets per sensor\n",
    "\n",
    "# Combine all test categories per file\n",
    "for i in range(num_files):\n",
    "    file_dfs = []\n",
    "    if i < len(normal_test) and not normal_test[i].empty: file_dfs.append(normal_test[i])\n",
    "    if i < len(botnet_test) and not botnet_test[i].empty: file_dfs.append(botnet_test[i])\n",
    "    if i < len(botnet_spam_test) and not botnet_spam_test[i].empty: file_dfs.append(botnet_spam_test[i])\n",
    "    \n",
    "    if file_dfs:\n",
    "        df = pd.concat(file_dfs, ignore_index=True)\n",
    "        # Shuffle the test data\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        test_dfs[loaded_keys[i]] = df # Store with the sensor key (e.g., '1', '2')\n",
    "\n",
    "del normal_test, botnet_test, botnet_spam_test\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Combined test data created (number of test sets: {len(test_dfs)})\")\n",
    "\n",
    "del i\n",
    "del file_dfs\n",
    "del df\n",
    "del num_files\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8f27ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying transformations (Proto dummies, Encoders) to all datasets...\n",
      "Processing DataFrame partition (Initial shape: (10345357, 12))\n",
      "  Shape after get_dummies: (10345357, 29)\n",
      "  Applying target data types...\n",
      "  Shape after finalizing columns and types: (10345357, 30)\n",
      "Processing DataFrame partition (Initial shape: (1468548, 12))\n",
      "  Shape after get_dummies: (1468548, 27)\n",
      "  Applying target data types...\n",
      "  Shape after finalizing columns and types: (1468548, 30)\n",
      "Processing DataFrame partition (Initial shape: (1799440, 12))\n",
      "  Shape after get_dummies: (1799440, 27)\n",
      "  Applying target data types...\n",
      "  Shape after finalizing columns and types: (1799440, 30)\n",
      "Processing DataFrame partition (Initial shape: (1165738, 12))\n",
      "  Shape after get_dummies: (1165738, 24)\n",
      "  Applying target data types...\n",
      "  Shape after finalizing columns and types: (1165738, 30)\n",
      "All transformations complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all dataframes (train + tests) to be processed together\n",
    "all_dfs_to_transform = [train_df] + list(test_dfs.values())\n",
    "del train_df # Save memory, train_df is already in the list\n",
    "gc.collect()\n",
    "\n",
    "final_dfs_list = [] # List to store the final results\n",
    "\n",
    "# Define the target columns\n",
    "protocol_cols_from_target = [\n",
    "    'arp', 'esp', 'gre', 'icmp', 'igmp', 'ipv6', 'ipv6-icmp', 'ipx/spx', 'llc',\n",
    "    'pim', 'rarp', 'rsvp', 'rtcp', 'rtp', 'tcp', 'udp', 'udt', 'unas', 'ipnip'\n",
    "]\n",
    "core_cols = ['Dur', 'SrcAddr', 'Sport', 'Dir', 'DstAddr', 'Dport', 'State',\n",
    "             'TotPkts', 'TotBytes', 'SrcBytes', 'Label']\n",
    "final_column_order = core_cols + protocol_cols_from_target\n",
    "\n",
    "print(\"Applying transformations (Proto dummies, Encoders) to all datasets...\")\n",
    "\n",
    "for df_orig in all_dfs_to_transform:\n",
    "    df = df_orig.copy()\n",
    "    print(f\"Processing DataFrame partition (Initial shape: {df.shape})\")\n",
    "\n",
    "    # 1. One-Hot Encoding for 'Proto'\n",
    "    if 'Proto' in df.columns:\n",
    "        df = pd.get_dummies(df, columns=['Proto'], prefix='', prefix_sep='')\n",
    "        print(f\"  Shape after get_dummies: {df.shape}\")\n",
    "\n",
    "    # 2. Apply LabelEncoder (Transform)\n",
    "    for col in categorical_cols:\n",
    "         if col in encoders: # If the column is in our encoder\n",
    "            df[col] = df[col].fillna('nan').astype(str)\n",
    "            \n",
    "            # Separate known (from training) and unknown values\n",
    "            known_mask = df[col].isin(encoders[col].classes_)\n",
    "            known_indices = df.index[known_mask]\n",
    "            unknown_indices = df.index[~known_mask]\n",
    "            \n",
    "            # Transform known values\n",
    "            if not known_indices.empty:\n",
    "                df.loc[known_indices, col] = encoders[col].transform(df.loc[known_indices, col])\n",
    "            \n",
    "            # Impute unknown values with the encoded 'nan' value\n",
    "            unknown_fill_value = imputation_values.get(col, 0) # Get the 'nan' ID\n",
    "            if not unknown_indices.empty:\n",
    "                df.loc[unknown_indices, col] = unknown_fill_value\n",
    "                \n",
    "         elif col in df.columns: # If column exists but wasn't encoded\n",
    "             # Fill with the default imputation value\n",
    "             df[col] = imputation_values.get(col, 0) \n",
    "\n",
    "    # 3. Ensure all columns (including dummies) exist\n",
    "    for col in final_column_order:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0 # Add missing dummy/protocol columns\n",
    "\n",
    "    # 4. Set column order and take only required columns\n",
    "    df_final = df[final_column_order].copy()\n",
    "\n",
    "    # 5. Apply target data types\n",
    "    print(f\"  Applying target data types...\")\n",
    "    for col in df_final.columns:\n",
    "        if col == 'Dur':\n",
    "            df_final[col] = pd.to_numeric(df_final[col], errors='coerce').fillna(0).astype(np.float64)\n",
    "        elif col == 'Label':\n",
    "            df_final[col] = df_final[col].astype(object) # Label remains a string\n",
    "        else:\n",
    "            # All other columns (encoded IDs and dummies)\n",
    "            df_final[col] = pd.to_numeric(df_final[col], errors='coerce').fillna(0).astype(np.int64)\n",
    "\n",
    "    print(f\"  Shape after finalizing columns and types: {df_final.shape}\")\n",
    "    final_dfs_list.append(df_final)\n",
    "    \n",
    "    del df, df_orig, df_final\n",
    "    gc.collect()\n",
    "\n",
    "print(\"All transformations complete.\")\n",
    "\n",
    "del all_dfs_to_transform\n",
    "del col, categorical_cols, known_mask, known_indices, unknown_indices, unknown_fill_value, imputation_values\n",
    "del protocol_cols_from_target, core_cols, final_column_order\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "489398c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved ENCODED training data to final_dataset\\train.csv (shape: (10345357, 30))\n",
      "Successfully saved ENCODED test data to final_dataset\\test_1.csv (shape: (1468548, 30))\n",
      "Successfully saved ENCODED test data to final_dataset\\test_2.csv (shape: (1799440, 30))\n",
      "Successfully saved ENCODED test data to final_dataset\\test_3.csv (shape: (1165738, 30))\n",
      "\n",
      "--- Processing and encoding complete! ---\n",
      "Final files are in the 'final_dataset' directory.\n"
     ]
    }
   ],
   "source": [
    "# Separate the processed list back into train and test\n",
    "final_train_df = final_dfs_list[0]\n",
    "final_test_dfs = {key: df for key, df in zip(test_dfs.keys(), final_dfs_list[1:])}\n",
    "\n",
    "# 1. Save the training data\n",
    "train_output_file = os.path.join(output_dir, 'train.csv')\n",
    "final_train_df.to_csv(train_output_file, index=False)\n",
    "print(f\"Successfully saved ENCODED training data to {train_output_file} (shape: {final_train_df.shape})\")\n",
    "\n",
    "# 2. Save all test data\n",
    "for key, df in final_test_dfs.items():\n",
    "    test_output_file = os.path.join(output_dir, f\"test_{key}.csv\")\n",
    "    df.to_csv(test_output_file, index=False)\n",
    "    print(f\"Successfully saved ENCODED test data to {test_output_file} (shape: {df.shape})\")\n",
    "\n",
    "print(\"\\n--- Processing and encoding complete! ---\")\n",
    "print(f\"Final files are in the '{output_dir}' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
