{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76527031",
   "metadata": {},
   "source": [
    "- This file is used for making train data (1 file) and test data (5 file) from all sensor\n",
    "- The training data will consist of an accumulation of 70% from each sensor\n",
    "- The test data will consist of 30% from each sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0151502b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting combined data processing and encoding script [CTU-13 v15 - Standalone, Target Dtypes]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "print(\"Starting combined data processing and encoding script [CTU-13 v15 - Standalone, Target Dtypes]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c48d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'final_dataset'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "# --- Paths for CTU-13 dataset ---\n",
    "dataset_files = {\n",
    "    '1': '1/capture20110810.binetflow',\n",
    "    '2': '2/capture20110811.binetflow',\n",
    "    '5': '5/capture20110815-2.binetflow',\n",
    "    '9': '9/capture20110817.binetflow',\n",
    "    '13': '13/capture20110815-3.binetflow'\n",
    "}\n",
    "# --- End paths ---\n",
    "\n",
    "# Initialize a list to hold data\n",
    "df_list = []\n",
    "loaded_keys = []\n",
    "all_original_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5f7224d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all raw CTU-13 dataframes for encoder fitting...\n",
      "Successfully loaded: 1/capture20110810.binetflow (key: 1, shape: (2824636, 15))\n",
      "Successfully loaded: 2/capture20110811.binetflow (key: 2, shape: (1808122, 15))\n",
      "Successfully loaded: 5/capture20110815-2.binetflow (key: 5, shape: (129832, 15))\n",
      "Successfully loaded: 9/capture20110817.binetflow (key: 9, shape: (2087508, 15))\n",
      "Successfully loaded: 13/capture20110815-3.binetflow (key: 13, shape: (1925149, 15))\n",
      "Total 5 dataframes loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading all raw CTU-13 dataframes for encoder fitting...\")\n",
    "for key, path in dataset_files.items():\n",
    "    try:\n",
    "        dtype_spec = {'sTos': str, 'dTos': str}\n",
    "        df = pd.read_csv(path, dtype=dtype_spec)\n",
    "        \n",
    "        # Explicit handling of sTos/dTos data types\n",
    "        if 'dTos' in df.columns:\n",
    "            df['dTos'] = pd.to_numeric(df['dTos'], errors='coerce').fillna(0)\n",
    "        if 'sTos' in df.columns:\n",
    "            df['sTos'] = pd.to_numeric(df['sTos'], errors='coerce').fillna(0)\n",
    "            \n",
    "        df_list.append(df)\n",
    "        all_original_data.append(df.copy()) # Save the original copy for encoder fitting\n",
    "        loaded_keys.append(key)\n",
    "        print(f\"Successfully loaded: {path} (key: {key}, shape: {df.shape})\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Dataset file not found at {path}. Skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load {path}. Error: {e}. Skipping.\")\n",
    "\n",
    "if not df_list:\n",
    "    print(\"Error: No .binetflow datasets could be loaded. Stopping script.\")\n",
    "    # In a notebook, you might want to raise an Error instead of using exit()\n",
    "    # raise FileNotFoundError(\"No .binetflow datasets could be loaded.\")\n",
    "else:\n",
    "    print(f\"Total {len(df_list)} dataframes loaded.\")\n",
    "\n",
    "del dtype_spec\n",
    "del dataset_files\n",
    "del path\n",
    "del key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5348b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LabelEncoders on combined raw CTU-13 data...\n",
      "Fitting LabelEncoder for: SrcAddr\n",
      "  No NaN found, using fallback ID: 0\n",
      "Fitting LabelEncoder for: Sport\n",
      "  Found NaN, its encoded ID is: 64905\n",
      "Fitting LabelEncoder for: Dir\n",
      "  No NaN found, using fallback ID: 0\n",
      "Fitting LabelEncoder for: DstAddr\n",
      "  No NaN found, using fallback ID: 0\n",
      "Fitting LabelEncoder for: Dport\n",
      "  Found NaN, its encoded ID is: 88434\n",
      "Fitting LabelEncoder for: State\n",
      "  Found NaN, its encoded ID is: 327\n",
      "Encoders are ready.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Fitting LabelEncoders on combined raw CTU-13 data...\")\n",
    "combined_fit_df = pd.concat(all_original_data, ignore_index=True)\n",
    "del all_original_data # Delete the original merged data to save memory\n",
    "gc.collect()\n",
    "\n",
    "categorical_cols = ['SrcAddr', 'Sport', 'Dir', 'DstAddr', 'Dport', 'State']\n",
    "encoders = {}\n",
    "imputation_values = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in combined_fit_df.columns:\n",
    "        print(f\"Fitting LabelEncoder for: {col}\")\n",
    "        # Fill NaN with the string 'nan' so it can be encoded\n",
    "        combined_fit_df[col] = combined_fit_df[col].fillna('nan').astype(str)\n",
    "        unique_values = combined_fit_df[col].unique()\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        le.fit(unique_values)\n",
    "        encoders[col] = le\n",
    "        \n",
    "        # Simpan nilai imputasi untuk NaN\n",
    "        if 'nan' in le.classes_:\n",
    "            imputation_values[col] = int(le.transform(['nan'])[0])\n",
    "            print(f\"  Found NaN, its encoded ID is: {imputation_values[col]}\")\n",
    "        else:\n",
    "            # Fallback if 'nan' is not present (even though it should be)\n",
    "            imputation_values[col] = 0 \n",
    "            print(f\"  No NaN found, using fallback ID: {imputation_values[col]}\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found in combined data. Skipping encoding for it.\")\n",
    "\n",
    "print(\"Encoders are ready.\")\n",
    "del combined_fit_df # Delete the merged data after fitting\n",
    "del unique_values\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98074a45",
   "metadata": {},
   "source": [
    "To make the data simpler, (following my paper) this code will remove the unnecessary feature such as:\n",
    "- dTos\n",
    "- sTos\n",
    "- ActivityLabel (only in NCC-2)\n",
    "- BotnetName (only in NCC-2)\n",
    "- SensorId (only in NCC-2)\n",
    "- StartTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba62698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied label simplification.\n",
      "Dropped unnecessary columns (if they existed).\n"
     ]
    }
   ],
   "source": [
    "# Function to simplify labels\n",
    "def categorize_label(label):\n",
    "    label_str = str(label).lower()\n",
    "    if 'botnet' in label_str:\n",
    "        if 'spam' in label_str: return 'botnet_spam'\n",
    "        else: return 'botnet'\n",
    "    elif 'background' in label_str or 'normal' in label_str: return 'normal'\n",
    "    else: return 'normal' # Assume the other labels as normal\n",
    "\n",
    "# Apply label simplification\n",
    "for i in range(len(df_list)):\n",
    "    df_list[i]['Label'] = df_list[i]['Label'].apply(categorize_label)\n",
    "print(\"Applied label simplification.\")\n",
    "\n",
    "# Remove unnecessary columnss\n",
    "columns_to_drop = ['dTos', 'sTos', 'ActivityLabel', 'BotnetName', 'SensorId', 'StartTime']\n",
    "for i in range(len(df_list)):\n",
    "    cols_to_drop_existing = [col for col in columns_to_drop if col in df_list[i].columns]\n",
    "    if cols_to_drop_existing:\n",
    "        df_list[i] = df_list[i].drop(columns=cols_to_drop_existing, errors='ignore')\n",
    "print(f\"Dropped unnecessary columns (if they existed).\")\n",
    "\n",
    "del columns_to_drop\n",
    "del cols_to_drop_existing\n",
    "del col, i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8aeb6",
   "metadata": {},
   "source": [
    "Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4899768c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed stratified train-test split.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate by label for manual stratified split per file\n",
    "normal_train, normal_test = [], []\n",
    "botnet_train, botnet_test = [], []\n",
    "botnet_spam_train, botnet_spam_test = [], []\n",
    "normal_df, botnet_df, botnet_spam_df = [], [], []\n",
    "\n",
    "for df in df_list:\n",
    "    normal_df.append(df[df['Label'] == 'normal'])\n",
    "    botnet_df.append(df[df['Label'] == 'botnet'])\n",
    "    botnet_spam_df.append(df[df['Label'] == 'botnet_spam'])\n",
    "del df_list # Delete the original df_list\n",
    "gc.collect()\n",
    "\n",
    "# Perform a split on each part\n",
    "for i in range(len(loaded_keys)):\n",
    "    if i < len(normal_df) and not normal_df[i].empty:\n",
    "        tr, te = train_test_split(normal_df[i], test_size=0.3, random_state=42)\n",
    "        normal_train.append(tr); normal_test.append(te)\n",
    "    else:\n",
    "        normal_train.append(pd.DataFrame()); normal_test.append(pd.DataFrame())\n",
    "        \n",
    "    if i < len(botnet_df) and not botnet_df[i].empty:\n",
    "        tr, te = train_test_split(botnet_df[i], test_size=0.3, random_state=42)\n",
    "        botnet_train.append(tr); botnet_test.append(te)\n",
    "    else:\n",
    "        botnet_train.append(pd.DataFrame()); botnet_test.append(pd.DataFrame())\n",
    "\n",
    "    if i < len(botnet_spam_df) and not botnet_spam_df[i].empty:\n",
    "        tr, te = train_test_split(botnet_spam_df[i], test_size=0.3, random_state=42)\n",
    "        botnet_spam_train.append(tr); botnet_spam_test.append(te)\n",
    "    else:\n",
    "        botnet_spam_train.append(pd.DataFrame()); botnet_spam_test.append(pd.DataFrame())\n",
    "\n",
    "print(\"Performed stratified train-test split.\")\n",
    "del normal_df, botnet_df, botnet_spam_df\n",
    "del tr, te, i\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0145740b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined training data (shape: (6142665, 12))\n"
     ]
    }
   ],
   "source": [
    "# Combine all training data\n",
    "temp_train_df = []\n",
    "num_files = len(loaded_keys)\n",
    "for i in range(num_files):\n",
    "    file_dfs = []\n",
    "    if i < len(normal_train) and not normal_train[i].empty: file_dfs.append(normal_train[i])\n",
    "    if i < len(botnet_train) and not botnet_train[i].empty: file_dfs.append(botnet_train[i])\n",
    "    if i < len(botnet_spam_train) and not botnet_spam_train[i].empty: file_dfs.append(botnet_spam_train[i])\n",
    "    \n",
    "    if file_dfs:\n",
    "        temp_train_df.append(pd.concat(file_dfs, ignore_index=True))\n",
    "\n",
    "del normal_train, botnet_train, botnet_spam_train\n",
    "del i\n",
    "gc.collect()\n",
    "\n",
    "if not temp_train_df:\n",
    "    print(\"Error: No training data was generated.\")\n",
    "    # raise ValueError(\"No training data was generated.\")\n",
    "else:\n",
    "    train_df = pd.concat(temp_train_df, ignore_index=True)\n",
    "    del temp_train_df\n",
    "    gc.collect()\n",
    "    # Shuffle the training data\n",
    "    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Combined training data (shape: {train_df.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f060b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined test data (number of test sets: 5)\n"
     ]
    }
   ],
   "source": [
    "# Combine test data (still separated by original file)\n",
    "test_dfs = {}\n",
    "for i in range(num_files):\n",
    "    file_dfs = []\n",
    "    if i < len(normal_test) and not normal_test[i].empty: file_dfs.append(normal_test[i])\n",
    "    if i < len(botnet_test) and not botnet_test[i].empty: file_dfs.append(botnet_test[i])\n",
    "    if i < len(botnet_spam_test) and not botnet_spam_test[i].empty: file_dfs.append(botnet_spam_test[i])\n",
    "    \n",
    "    if file_dfs:\n",
    "        df = pd.concat(file_dfs, ignore_index=True)\n",
    "        # Shuffle test data per file\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        test_dfs[loaded_keys[i]] = df\n",
    "\n",
    "del normal_test, botnet_test, botnet_spam_test\n",
    "del i\n",
    "del file_dfs\n",
    "del df\n",
    "del num_files\n",
    "gc.collect()\n",
    "print(f\"Combined test data (number of test sets: {len(test_dfs)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4fa5fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying transformations (Proto dummies, fitted Encoders) to all datasets...\n",
      "Processing DataFrame partition (Initial shape: (6142665, 12))\n",
      "  Shape after get_dummies: (6142665, 30)\n",
      "  Applying target data types...\n",
      "  Shape after finalizing columns and types: (6142665, 30)\n",
      "Processing DataFrame partition (Initial shape: (847393, 12))\n",
      "  Shape after get_dummies: (847393, 26)\n",
      "  Applying target data types...\n",
      "  Shape after finalizing columns and types: (847393, 30)\n",
      "Processing DataFrame partition (Initial shape: (542438, 12))\n",
      "  Shape after get_dummies: (542438, 22)\n",
      "  Applying target data types...\n",
      "  Shape after finalizing columns and types: (542438, 30)\n",
      "Processing DataFrame partition (Initial shape: (38951, 12))\n",
      "  Shape after get_dummies: (38951, 21)\n",
      "  Applying target data types...\n",
      "  Shape after finalizing columns and types: (38951, 30)\n",
      "Processing DataFrame partition (Initial shape: (626254, 12))\n",
      "  Shape after get_dummies: (626254, 25)\n",
      "  Applying target data types...\n",
      "  Shape after finalizing columns and types: (626254, 30)\n",
      "Processing DataFrame partition (Initial shape: (577546, 12))\n",
      "  Shape after get_dummies: (577546, 24)\n",
      "  Applying target data types...\n",
      "  Shape after finalizing columns and types: (577546, 30)\n",
      "Transformations complete.\n"
     ]
    }
   ],
   "source": [
    "all_dfs_to_transform = [train_df] + list(test_dfs.values())\n",
    "del train_df # Delete the train_df variable, it will be replaced later\n",
    "gc.collect()\n",
    "\n",
    "final_dfs_list = []\n",
    "\n",
    "print(\"Applying transformations (Proto dummies, fitted Encoders) to all datasets...\")\n",
    "for df_orig in all_dfs_to_transform:\n",
    "    df = df_orig.copy()\n",
    "    print(f\"Processing DataFrame partition (Initial shape: {df.shape})\")\n",
    "\n",
    "    # 1. Create dummies for 'Proto\n",
    "    if 'Proto' in df.columns:\n",
    "        df = pd.get_dummies(df, columns=['Proto'], prefix='', prefix_sep='')\n",
    "        print(f\"  Shape after get_dummies: {df.shape}\")\n",
    "\n",
    "    # 2. Apply (transform) the encoders that have been fitted\n",
    "    for col in categorical_cols:\n",
    "        if col in encoders:\n",
    "            df[col] = df[col].fillna('nan').astype(str)\n",
    "            # Separate data with values 'known' vs 'unknown'\n",
    "            known_mask = df[col].isin(encoders[col].classes_)\n",
    "            known_indices = df.index[known_mask]\n",
    "            unknown_indices = df.index[~known_mask]\n",
    "            \n",
    "            # Transform the known values\n",
    "            if not known_indices.empty:\n",
    "                df.loc[known_indices, col] = encoders[col].transform(df.loc[known_indices, col])\n",
    "            \n",
    "            # Impute unknown values (including NaN)\n",
    "            unknown_fill_value = imputation_values.get(col, 0) # Retrieve the imputed NaN values\n",
    "            if not unknown_indices.empty:\n",
    "                df.loc[unknown_indices, col] = unknown_fill_value\n",
    "        elif col in df.columns:\n",
    "            # If the column exists but the encoder does not (this should not happen)\n",
    "            df[col] = imputation_values.get(col, 0)\n",
    "\n",
    "    # 3. Standardize Columns and Data Types\n",
    "    protocol_cols_from_target = [\n",
    "        'arp', 'esp', 'gre', 'icmp', 'igmp', 'ipv6', 'ipv6-icmp', 'ipx/spx', 'llc',\n",
    "        'pim', 'rarp', 'rsvp', 'rtcp', 'rtp', 'tcp', 'udp', 'udt', 'unas', 'ipnip'\n",
    "    ]\n",
    "    core_cols = ['Dur', 'SrcAddr', 'Sport', 'Dir', 'DstAddr', 'Dport', 'State',\n",
    "                 'TotPkts', 'TotBytes', 'SrcBytes', 'Label']\n",
    "    final_column_order = core_cols + protocol_cols_from_target\n",
    "\n",
    "    # Add any missing columns (from get_dummies)\n",
    "    for col in final_column_order:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0 # Default to 0\n",
    "\n",
    "    df_final = df[final_column_order].copy() # Ensure the column order is correct\n",
    "\n",
    "    # Apply the target data type\n",
    "    print(f\"  Applying target data types...\")\n",
    "    for col in df_final.columns:\n",
    "        if col == 'Dur':\n",
    "            df_final[col] = pd.to_numeric(df_final[col], errors='coerce').fillna(0).astype(np.float64)\n",
    "        elif col == 'Label':\n",
    "            df_final[col] = df_final[col].astype(object) # Keep the label as a string\n",
    "        else:\n",
    "            # All other columns (including encoded and dummies) become int64\n",
    "            df_final[col] = pd.to_numeric(df_final[col], errors='coerce').fillna(0).astype(np.int64)\n",
    "\n",
    "    print(f\"  Shape after finalizing columns and types: {df_final.shape}\")\n",
    "    final_dfs_list.append(df_final)\n",
    "    del df, df_orig, df_final\n",
    "    gc.collect()\n",
    "\n",
    "del all_dfs_to_transform\n",
    "del col, categorical_cols, known_mask, known_indices, unknown_indices, unknown_fill_value, imputation_values\n",
    "del protocol_cols_from_target, core_cols, final_column_order\n",
    "gc.collect()\n",
    "\n",
    "print(\"Transformations complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d54b0bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ENCODED training data to final_dataset\\train.csv (shape: (6142665, 30))\n",
      "Saved ENCODED test data to final_dataset\\test_1.csv (shape: (847393, 30))\n",
      "Saved ENCODED test data to final_dataset\\test_2.csv (shape: (542438, 30))\n",
      "Saved ENCODED test data to final_dataset\\test_5.csv (shape: (38951, 30))\n",
      "Saved ENCODED test data to final_dataset\\test_9.csv (shape: (626254, 30))\n",
      "Saved ENCODED test data to final_dataset\\test_13.csv (shape: (577546, 30))\n",
      "\n",
      "--- All-in-one processing and encoding complete for CTU-13! ---\n",
      "Final files are in the 'final_dataset' directory.\n"
     ]
    }
   ],
   "source": [
    "# Pisahkan kembali hasil akhir\n",
    "final_train_df = final_dfs_list[0]\n",
    "final_test_dfs = {key: df for key, df in zip(test_dfs.keys(), final_dfs_list[1:])}\n",
    "\n",
    "# Simpan data training\n",
    "train_output_file = os.path.join(output_dir, 'train.csv')\n",
    "final_train_df.to_csv(train_output_file, index=False)\n",
    "print(f\"Saved ENCODED training data to {train_output_file} (shape: {final_train_df.shape})\")\n",
    "\n",
    "# Simpan semua data test\n",
    "for key, df in final_test_dfs.items():\n",
    "    test_output_file = os.path.join(output_dir, f\"test_{key}.csv\")\n",
    "    df.to_csv(test_output_file, index=False)\n",
    "    print(f\"Saved ENCODED test data to {test_output_file} (shape: {df.shape})\")\n",
    "\n",
    "print(\"\\n--- All-in-one processing and encoding complete for CTU-13! ---\")\n",
    "print(f\"Final files are in the '{output_dir}' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
